{
    "nbformat": 4, 
    "nbformat_minor": 0, 
    "cells": [
        {
            "source": "<img src='https://raw.githubusercontent.com/dxkikuchi/SparkSnips/master/dsxbanner.jpg' width='75%'></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "IBM Data Science Experience is an interactive, collaborative, cloud-based environment where data scientists can use multiple tools to activate their insights.  Data scientists can use the best of open source, tap into IBM's unique features, grow their capabilities, and share their successes.  In addition to all the features in the current preview, many new capabilities are being added including the ability to ingest Object Storage data with a single click, an enhanced user interface for version control, a facility to comment or chat about a notebook with others, and many more!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## New York State Restaurant Inspections Notebook\nThis notebook will provide insights from official restaurant inspection records for most of New York state and provide visualizations of that data.  This data is available at <a href=\"https://health.data.ny.gov/Health/Food-Service-Establishment-Last-Inspection/cnih-y5dw\" target=\"_blank\">New York State Food Service Establishment: Last Inspection</a>.  A raw extract was taken in October of 2016 and is located at http://ibm.biz/nyrestaurantsdata.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "nyrdata = 'http://ibm.biz/nyrestaurantsdata'"
        }, 
        {
            "source": "The csv (comma separated values) data will be read into a Pandas dataframe (nyr) and the first 5 records are displayed using the 'head()' method.<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import pandas as pd\nnyr = pd.read_csv(nyrdata)\nnyr.head()"
        }, 
        {
            "source": "Ingesting data can be as simple as using one line of code.  Similarly, data can be ingested from Cloudant, DashDB, Object Storage, relational databases, and many others.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Another dataframe will be created that will only contain the columns that are pertinent.  The 'head()' method will display the first 5 records of this dataframe.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "nyrcols = nyr[['FACILITY','TOTAL # CRITICAL VIOLATIONS','Location1']]\nnyrcols.head()"
        }, 
        {
            "source": "The data will be transformed into a Spark dataframe 'nyrDF' and a table will be registered.  Spark dataframes are conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimizations under the hood.  A table that is registered can be used in subsequent SQL statements.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "nyrDF = spark.createDataFrame(nyrcols)\nnyrDF.registerTempTable(\"nyrDF\")"
        }, 
        {
            "source": "Now, a Spark dataframe 'nyvDF' will be created using SQL that will contain the restaurant name (FACILITY), latitude, longitude and violations.  Note that the latitude and longitude are combined in the final column (Location1) of the retrieved data.  They will be extracted separately using regular expressions in the SQL.  The results are ordered by number of violations in descending order and the top 10 are displayed.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "query = \"\"\"\nselect \n    FACILITY, \n    trim(regexp_extract(location1, '(\\\\\\()(.*),(.*)(\\\\\\))',2)) as lat, \n    trim(regexp_extract(location1, '(\\\\\\()(.*),(.*)(\\\\\\))',3)) as lon,\n    cast(`TOTAL # CRITICAL VIOLATIONS` as int) as Violations\nfrom nyrDF \norder by Violations desc\nlimit 1000\n\"\"\"\nnyvDF = sqlContext.sql(query)\nnyvDF.show(10)"
        }, 
        {
            "source": "Brunel visualization will be used to map the latitude and longitude to a New York state map.  Colors represent the number of violations as noted in the key.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import brunel\nnyvPan = nyvDF.toPandas()\n%brunel map ('NY') + data('nyvPan') x(lon) y(lat) color(Violations) tooltip(FACILITY)"
        }, 
        {
            "source": "One of the many key strengths of Data Science Experience is the ability to easily search and quickly learn about various topics.  For example, to find articles, tutorials or notebooks on Brunel, click on the 'link' icon on the top right hand corner of this web page ('Find Resources in the Commuity').  A side palette will appear where you can enter 'Brunel' or other topics of interest.  Related articles, tutorials, notebooks, data cards will appear.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Pixiedust provides charting and visualization.  It is an open source Python library that works as an add-on to Jupyter notebooks to improve the user experience of working with data.  Please execute the next cell for a tabular view of the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "dataframe"
                    }
                }, 
                "collapsed": false, 
                "scrolled": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "from pixiedust.display import *\ndisplay(nyvDF)"
        }, 
        {
            "source": "If you hover over the lonely lighter colored dot in the middle of the New York State map, you can see that it is for 'CAMP KINGSLEY - CC'.  By starting to type the value 'camp' in the 'Search table' text field above, the record will be displayed.  Numerous visualization are available with support for maps in the future.  Please take a look at the histogram of this data for another insight.  In addition, the data can be downloaded as a file, or stashed to Cloudant or Object Storage.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In just a few notebook cells, data was ingested, manipulated, visualized and yielded insights.  Much more capability, including machine learning, could be leveraged with IBM Data Science Experience.  This is just the tip of the iceberg!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Machine Learning allows computers to learn and find hidden insights without being explicitly programmed to do so and can change when exposed to new data.  It could relate to clustering, classification or collaborative filtering.  The following code calls a binomial logistic regression model that was created and deployed using Data Science Experience.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import requests\n\npayload = {\"record\":[\"WESTCHESTER\",\"Inspection\"]}\n\nresponse = requests.post('https://prod-scoring-ml.spark.bluemix.net:32768/v1/score/689', json=payload, headers=scoring_header)\n\nstatus = response.status_code\nstatement = response.text\n\nif status != 200:\n    raise Exception(\"Scoring endpoint prediction failed\", \"status code=\"+str(status), statement)\n    \nprint statement"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": ""
        }
    ], 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }
    }
}